---
title: "LC-MS data processing using *xcms*"
format:
  html:
    toc: true
    self-contained: true
author: "Marilyn De Graeve, Philippine Louail, Johannes Rainer"
code_folding: hide
editor: source
date: 'Compiled: `r format(Sys.Date(), "%B %d, %Y")`'
bibliography: lcms-data-preprocessing.bib
csl: lcms-data-preprocessing.csl
---

```{r setup, include=FALSE}
library(knitr)
library(BiocStyle)
knitr::opts_knit$set(root.dir = './')
```

# Abstract

*#FINDME 200 words. open access, teaser. @phili: ok?**

Liquid chromatography-mass spectrometry (LC-MS) is a cornerstone technology in
metabolomics, enabling high-throughput detection of small molecules across
diverse biological samples. However, raw LC-MS data are complex, requiring
careful preprocessing to ensure accurate and reproducible feature
detection. This chapter introduces a step-by-step protocol for LC-MS data
preprocessing using the open-source *xcms* package in R. Designed for users
ranging from beginners to experienced analysts, the chapter outlines critical
stages including data inspection, peak detection, retention time alignment,
correspondence, and result export. Special attention is given to parameter
optimization and diagnostic visualization to guide users in making informed
decisions tailored to their specific data set. We demonstrate the approach on
human serum samples, using real-life example compounds such as proline to
showcase retention time alignment and peak correspondence. By combining
practical code snippets with conceptual insights, this chapter empowers
researchers to harness *xcms* for robust, reproducible LC-MS workflows in
untargeted metabolomics. Whether you are setting up your first analysis or
refining an established pipeline, this chapter serves as both a tutorial and a
reference for high-quality LC-MS data processing.

# Key words

*xcms*, Liquid chromatography, Mass spectrometry, Metabolomics, Data processing,
Preprocessing, Peak detection, Retention time correction, Quality control

# 1. Introduction

Liquid chromatography-mass spectrometry (LC-MS) has become a cornerstone
technique in metabolomics and proteomics due to its
high sensitivity, selectivity, and ability to detect a broad range of compounds
in complex biological samples. However, the raw data generated by LC-MS
instruments is complex and multidimensional, requiring careful preprocessing to
extract meaningful and reproducible quantitative information. Key steps of the
preprocessing typically include chromatographic peak detection, retention time
alignment, correspondence analysis, and gap filling. 

Liquid chromatography-mass spectrometry (LC-MS) has become a cornerstone
technique in metabolomics and proteomics due to its high sensitivity,
selectivity, and ability to detect a broad range of compounds in complex
biological samples. However, the raw data generated by LC-MS instruments is
complex and multidimensional, requiring careful preprocessing to extract
meaningful and reproducible quantitative information. Key steps of the
preprocessing typically include chromatographic peak detection, retention time
alignment, correspondence analysis, and gap filling.  Preprocessing of LC-MS
data is highly influenced by the specific LC setup and MS system
used. LC-dependent influences include the type and length of the chromatography
column, particle size, flow rate, gradient composition, and the chemical nature
of the mobile phase (solvents, pH, buffer salts)[@lindahl_tuning_2017]. These
parameters affect chromatographic resolution, retention time stability (i.e.,
batch effects), peak shape, and the degree of co-elution between analytes
[@liu_addressing_2020]. Additionally, matrix effects arising from the sample
type, such as serum, can influence retention and suppression or enhancement of
signals [@fang_matrix_2015].  MS-dependent influences relate to the instrument
type (e.g., TOF, Orbitrap), ion source settings, acquisition mode (centroid
vs. profile, DDA vs. DIA), mass accuracy, and resolution. These factors affect
the sensitivity, dynamic range, and spectral complexity of the acquired data,
and have a direct impact on the mass deviation tolerance
[@vereyken_highresolution_2019].  Therefore, parameters for each of the
preprocessing analysis steps need to be adjusted based on the experimental setup
as well as instrument configurations. A visual as well as quantitative
evaluation of the data before - and during the analysis is crucial to ensure
high quality results.

This chapter introduces the *xcms* R package [@temp_xcms_preprint], a widely
adopted, open-source tool designed for LC-MS data preprocessing and
analysis. Developed with a strong foundation in statistical computing and
reproducible research principles, *xcms* provides robust and flexible algorithms
for key tasks such as peak detection, retention time correction and feature
alignment across samples. Its seamless integration with the broader R ecosystem
allows for the entire workflow, from initial inspection to normalization and
downstream statistical analysis, to be executed within a single, reproducible
environment. Moreover, *xcms* is optimized to handle computationally intensive,
big data sets, making it suitable for both exploratory and large-scale studies.

In detail, we will demonstrate a complete, reproducible LC-MS data preprocessing
workflow using *xcms*, illustrated with a small untargeted LC-MS metabolomics
data set. Special emphasis is placed on the best practices for data handling,
parameter optimization, and quality control. By the end of this chapter, readers
will have a strong foundation in using *xcms* to transform raw LC-MS data into
structured high-quality feature tables suitable for downstream statistical
analysis and biological interpretation.

# 2. Material

This provides an overview of the data set, software environment, and workflow
used for LC-MS(/MS) data preprocessing with the *xcms* R package. The entire
analysis is designed to be reproducible and made publicly available.

## 2.1 Overview of the workflow

To make the data processing steps transparent and reproducible, the analysis
performed in this chapter is also implemented as a Quarto document
(.qmd). Quarto enables seamless integration of code, narrative, and output,
allowing readers to execute and modify the workflow directly (*see* **Note
1**). While any code editor may be used, [RStudio](https://rstudio.org) offers
built-in support for both R and Quarto and is a commonly used interface in data
science.

As LC-MS preprocessing is strongly influenced by the specific setup used, there
is no single set of parameters that fits all configurations, hence, settings for
each preprocessing algorithm need to be defined and evaluated on the actual
experimental data. This workflow shows examples to derive data set-specific
settings for each preprocessing step and tools to evaluate their impact.
Depending on the data it might be needed to test and evaluate different settings
to adapt them to the experiment.

## 2.2 Data set

The LC-MS data used in this chapter comes from a small, publicly available
untargeted metabolomics study. Samples were analyzed using HILIC UPLC-QTOF-MS in
positive ionization modes, as described in *XXX [FINDME @jo:
REF to paper LC-MS METHOD]*. Raw data files are provided in the open and
vendor-independent .mzML format (*see* **Note 2**).

The data set consists of pooled human serum samples, which represent internal
quality control (QC) samples, each spiked with a set of 15 compounds at two
concentration levels: high and low. Each sample was measured in the positive
ionisation mode and as a technical triplicate to assess reproducibility
(Table 1).

Additionally, two LC-MS/MS runs were acquired for one of the high concentration
samples using data-dependent acquisition (DDA) at two different collision
energies (20 eV and 30 eV). These are included to support downstream MS/MS-based
compound annotation (Table 1).

The data set is publicly available on MetaboLights (accession: *MTBLSXX [FINDME
@phili: deposit and add name]*) and can be downloaded from the archive for
use in this tutorial.

**Table 1**. Experimental design of the small untargeted metabolomics data set.

| mzML	| polarity	| time	| mode	| collision_energy	| concentration	| sample_name | sample_origin | sample_type |
|:-----:|:---------:|:-----:|:-----:|:-----------------:|:-------------:|:-----------:|:-------------:|:-----------:|
| QC_LowIS_Mix13_1_POS.mzML	| POS	| 2020-01-17 14:33:51	| MS1	| 	| low	| low_a | serum	| pool QC |
| QC_HighIS_Mix13_3_POS.mzML	| POS	| 2020-01-17 14:51:21	| MS1	| 	| high	| high_c | serum	| pool QC |
| QC_LowIS_Mix13_3_POS.mzML	| POS	| 2020-01-17 15:46:56	| MS1	| 	| low	| low_c | serum	| pool QC |
| QC_HighIS_Mix13_2_POS.mzML	| POS	| 2020-01-17 17:58:34	| MS1	| 	| high	| high_b | serum	| pool QC |
| QC_LowIS_Mix13_2_POS.mzML	| POS	| 2020-01-17 20:48:23	| MS1	| 	| low	| low_b | serum	| pool QC |
| QC_HighIS_Mix13_1_POS.mzML	| POS	| 2020-01-17 22:48:18	| MS1	| 	| high	| high_a | serum	| pool QC |
| QC_HighIS_Mix13_CE20_POS.mzML	| POS	| 2020-01-18 19:27:09	| DDA	| 20	| high	| high_20 | serum	| pool QC |
| QC_HighIS_Mix13_CE30_POS.mzML	| POS	| 2020-01-18 19:39:02	| DDA	| 30	| high	| high_30 | serum	| pool QC |

## 2.3 Software and R packages

The data analysis was performed using R (version 4.5.0), a statistical
programming language available for Windows and Unix (Linux, and macOS) systems.

The core LC-MS(/MS) preprocessing workflow is built upon the following R
packages:

- *Spectra* is used to import and handle the raw mass spectrometry data,
  providing efficient and flexible access to spectral information
  [@spectra].

- *MsExperiment* is employed to manage the experimental design and to link
  sample metadata with the corresponding raw data files [@ms_experiment].

- *xcms* performs key preprocessing steps including chromatographic peak
  detection, retention time alignment, and correspondence analysis
  [@temp_xcms_preprint].

Complementary packages that enhance and support mainly metabolomics-specific
analyses include:

- *MetaboCoreUtils* offers core utility functions for metabolomics data
  processing, including mass-to-charge (*m/z*) and retention time calculations,
  adduct annotation, and common data transformations used across multiple
  packages [@rainer_modular_2022].

- *SummarizedExperiment* provides Bioconductor's standardized container for
  storing assay data (e.g., feature intensities) alongside row (feature) and
  column (sample) metadata, facilitating consistent data handling and
  integration in metabolomics workflows [@summarized_experiment].

- *MsBackendMgf* implements support for reading and writing mass spectral data
  in the Mascot Generic Format (MGF), commonly used for storing MS/MS spectra
  in metabolomics and proteomics annotation tasks [@ms_backend_mgf].

All these packages are part of Bioconductor
[@gentleman_bioconductor_2004][@huber_orchestrating_2015] and can be installed
using the *BiocManager* R package (*see* **Note 3**).

Additional R packages support the workflow by enabling data visualization
(*RColorBrewer*, *pheatmap*), reading of metadata and tabular input files
(*readxl*), and providing utility functions that enhance reproducibility and
automation (*knitr*, *BiocStyle*).

## 2.4 Reproducible analysis workflow

To ensure complete reproducibility [@markowetz_five_2015] of the present
analysis, the workflow as well as the data are publicly available through GitHub
and MetaboLights respectively [FINDME REF]. Users can either manually install
all required dependencies, or load and run a Docker image that contains the
workflow, the data and all required software allowing interactive evaluation of
the code locally on the user's computer.

Information on the workflow files, data and installation instructions are
available at https://github.com/jorainer/xcms-preprocessing-protocol . [FINDME
@jo: rename repository]

# 3. Methods

Before the actual data processing, the experimental setup should be defined and
the data files/samples organized. The experimental setup is best defined using a
spreadsheet with one data files per row and columns containing the technical and
phenotypical properties of each data file respectively the sample. Ideally,
samples/files should be added in the order in which they were measured. For the
present analysis it is assumed that all MS data files are placed into a folder
*data/mzML*, and the spreadsheet with sample annotation to the folder *data*.

## 3.1 Data import

In this section, general setup steps are performed, including loading the
required R packages, configuring parallel processing, and specifying the data
directory. The sample metadata and raw LC-MS data are then loaded and
color-coded to facilitate clear and consistent visualization throughout the
analysis.

1) Load all required R/Bioconductor packages for the present analysis (*see*
   **Note 3**).

```{r}
#| message: false

#' Load all R packages
library(xcms)
library(MsExperiment)
library(Spectra)
library(readxl)
library(RColorBrewer)
library(pheatmap)
library(MetaboCoreUtils)
library(SummarizedExperiment)
library(MsBackendMgf)
```

2) Configure parallel processing. Configure the number of cores, using the
   `CORES` variable. All data analysis functions in *xcms* have built-in
   parallel processing capabilities. In addition, to reduce memory load, *xcms*
   performs analyses in a *chunk-wise manner*, i.e., only data from a certain
   number of samples is loaded and processed at a time. This can be configured
   with the parameter `chunkSize = ` in most data analysis
   functions. Importantly, the number of CPU cores used should be smaller or
   equal to the value for this parameter. Below we chose to use 2 CPU cores for
   parallel processing. R supports on Unix systems the *multi-core* parallel
   processing, while on Windows systems *simple network of workstation*
   (SNOW)-based parallel processing must be used.

```{r}
#' Define the number of cores
CORES <- 2

#' Set up parallel processing using 2 cores
if (.Platform$OS.type == "unix") {
    register(MulticoreParam(CORES))
} else {
    register(SnowParam(CORES))
}
```

3) Load experimental setup/sample information. It is good practice to define the
   experimental setup using e.g. a spreadsheet that contains the names of all
   of the experiment's data files (eventually including their path) and their
   respective samples including all phenotypical information as well as
   technical/experimental properties. Ideally, the order of the samples should
   match their injection sequence. The content of the spreadsheet for the
   present experiment is shown in Table 1. Load the experiment's spreadsheet
   using the `read_xlsx()` function from the *readxl* R package (*see*
   **Note 4**).

```{r}
#' Import sample file
sample_data <- read_xlsx("data/standards_mzml.xlsx") |> as.data.frame()
```

4) Create a *data representation* for the experiment. The `readMsExperiment()`
   function takes the names of the data files (including the path to the
   directory where they are stored, configured below using the `MZML_PATH`
   variable) and a `data.frame` with the sample information for the respective
   data files as input. The resulting `MsExperiment` object (variable
   `ms_data`) contains a reference to the MS data files and the sample
   information.

```{r}
#' Define the path to the directory where the data files can be found
MZML_PATH <- file.path("data/mzML/")

#' Linking the raw data to its respective metadata.
ms_data <- readMsExperiment(file.path(MZML_PATH, sample_data$mzML),
                            sampleData = sample_data)
```

5) Check if the *data representation* is correct by printing the sample
   information table. Sample information can be extracted using the
   `sampleData()` function.

```{r}
#| tbl-cap: "Data files and samples."

#' View the sample information table
sampleData(ms_data)[, c("mzML", "sample_name", "mode", "collision_energy",
                        "concentration")] |>
    kable(format = "pipe")
```

6) Define colors to label individual or groups of samples (*see* **Note 5**).

```{r}
#' Define colors by concentration levels: high and low
col_concentration <- c(low = "#FF7F00", high = "#A65628")
col_sample <- col_concentration[sampleData(ms_data)$concentration]
```

## 3.2 Initial data inspection

Before initiating the preprocessing of LC-MS data, it is essential to evaluate
the overall data quality and define key parameters. In this section, we perform
the visual inspection of the chromatographic base peak and extracted-ion
chromatograms (EICs), enabling the identification of poorly acquired samples,
retention time regions with meaningful signal, and potential contaminants. We
also verify whether the data was acquired in centroid mode, a requirement for
peak detection, and extract signal characteristics such as chromatographic peak
width and *m/z* mass deviation using the example compounds. While for the
present example we extract the EIC only for proline, a metabolite that is known
to be present in human serum samples, for real experiments signal for multiple
compounds or ions, ideally from retention time regions along the full retention
time range, should be extracted and their signal evaluated. These initial steps
help ensure that the data is suitable for automated processing and allow
optimization of parameter settings for robust peak detection and alignment.

### 3.2.1 Visualize base peak chromatogram

1) Create a base peak chromatogram (BPC) of the full data set and plot it. The
   BPC is plotted to assess chromatographic drift or shifts, sample quality or
   outliers, and the presence of systematic noise or chemical contamination
   (e.g., polymer peaks). With parameter `aggregationFun = "max"` the highest
   intensity per spectrum is reported, while setting `aggregationFun = "sum"`
   would sum-up all intensities per spectrum, hence generating a total ion
   chromatogram (TIC).

```{r}
#' Create a BPC
bpc <- chromatogram(ms_data, aggregationFun = "max", chunkSize = CORES)

#' Plot the BPC
plot(bpc, col = paste0(col_sample, 80))
grid()
abline(v = c(15, 225), lty = 2)
legend("topright", col = col_concentration, lty = 1,
       legend = names(col_concentration))
```

### 3.2.2 Retention time filtering

2) To focus analysis on the region where meaningful compound separation and
   detection occur, filter the data to a specific retention time window, e.g.,
   for the present data set, between 15 and 225 seconds. This can be guided by
   known elution times of spiked or expected endogenous compounds.

```{r}
#' Filter retention time
ms_data <- filterSpectra(ms_data, filterRt, c(15, 225))
```

3) Create a BPC of the retention time filtered data set for further evaluation
   under **Section 3.3.2**.

```{r}
#' Extract the BPC again - for reference later.
bpc <- chromatogram(ms_data, aggregationFun = "max", chunkSize = CORES)
```

### 3.2.3 Extracted-ion chromatogram selection

4) Determine the *m/z* range for proline. Starting from its chemical formula,
   calculate the theoretical mass and subsequently the theoretical *m/z* for one
   of its ions. We use `"[M+H]+"` as the most likely adduct of proline in
   positive polarity, but could also specify any other possible adduct, such as
   `"[M+Na]+`, `"[M+NH4]+` etc. Create a range around the theoretical *m/z* by
   subtracting and adding a value of 0.01 Da. Depending on the accuracy and
   precision of the MS instrument, this value might be increased or decreased.

```{r}
#' Calculate the mass of proline
proline_formula <- "C5H9NO2"
proline_mass <- calculateMass(proline_formula)

#' Calculate m/z for an adduct of proline
proline_mz <- mass2mz(proline_mass, c("[M+H]+"))[1, ]
proline_mzr <- cbind(mzmin = proline_mz - 0.01, mzmax = proline_mz + 0.01)
```

5) Extract and plot the EIC of proline.

```{r}
#' Extract the EIC for proline
proline_eic <- chromatogram(ms_data, mz = proline_mzr, rt = c(160, 200))

#' Plot the EIC for proline
plot(proline_eic, col = paste0(col_sample, 80), xlim = c(165, 175))
grid()
```

6) Repeat this for minimum 10 compounds and save the EIC objects. Inspect
   molecules that you have spiked (e.g., internal standards), are known to be
   endogenously present (e.g., amino acids in serum), or select unknown
   compounds (*see* **Note 6**). Be aware to select compouds with retention time
   and *m/z* values representing the full chromatogram and mass spectrum range,
   respectively. If the expected retention time range is unknown, omit the
   parameter `rt` of the `chromatogram()` function and extract the ion trace for
   the full full retention time range.

```{r}
#' Extract the EIC for an unknown compound
unknown_eic <- chromatogram(ms_data, mz = c(106.0399, 106.0599))

#' Plot the full EIC
plot(unknown_eic, col = paste0(col_sample, 80))
grid()
```

### 3.2.4 Check *m/z* data acquisition mode

7) Evaluate whether data is acquired in centroid mode in the first sample, using
   proline (*see* **Note 7**). In addition, determine the variance in peaks'
   *m/z* along the retention time axis for individual ions. For better
   readability of the code, the R *pipe operator* (`|>`) is used to
   concatenate subsequent function calls.

```{r}
#' Check if data is in centroid mode
ms_data_proline <- ms_data[1] |>
    filterSpectra(filterRt, c(167, 172)) |>
    filterSpectra(filterMzRange, proline_mzr[1, ])

plot(ms_data_proline)
```

## 3.3 Data preprocessing

In this section, we outline the essential steps of LC-MS data preprocessing
using the *xcms* package. Chromatographic peak detection identifies relevant
features in the raw data based on intensity patterns across retention time and
mass-to-charge ratio. Retention time shifts between measurement runs are then
adjusted in the retention time alignment step. The subsequent correspondence
analysis groups chromatographic peaks from different samples that represent the
same underlying compound defining thus the *LC-MS features*. This grouping
across samples relies on similarity of the chromatographic peak's *m/z* and
retention time values and depends thus on a successful reduction of retention
time shifts between the samples. Finally, gap filling recovers missing intensity
values where peaks were not initially detected, producing a complete and
consistent feature table ready for downstream statistical analysis.

The *xcms* package provides several different algorithms for each of these
steps. In the present example we use the most robust and commonly used
methods. Also, importantly, settings for these algorithms need to be adapted to
the analyzed data set. These are best defined using visual inspection of the
data and evaluation of the results, eventually requiring changing and optimizing
the parameters in several iterations.

### 3.3.1 Chromatographic peak detection

We will use the *centWave* algorithm [@tautenhahn_highly_2008] for
chromatographic peak detection. The most important parameters for this algorithm
are `peakwidth`, which defines the expected range of chromatographic peak widths
(in retention time dimension) in the data set, and `ppm` that defines the
maximal acceptable deviation (in *m/z* dimension) of mass peaks for a certain
ion in consecutive spectra. Other parameters for which different values than the
default are used are `snthresh` (signal-to-noise threshold) and `integrate`.
This parameter defines the trade-off between the number of false positive and
negative peaks. Parameter `integrate` allows to select the method to define the
chromatographic peaks boundaries in retention time dimension. The default
`integrate = 1` assumes symmetric, gaussian-shaped peaks, while `integrate = 2`
uses a method that defines peak boundaries through local minimas in peak
intensities.

1) Define the `peakwidth` parameter. Inspect extracted chromatographic peaks of
   the ≥ 10 plotted EICs and determine the average observed width of the peaks
   in retention time dimension. The rule of thumb is: choose an initial
   `peakwidth` to be between half of the observed peak width to maximum the
   double peak width (in seconds). Values `2` and `10` for the lower and upper
   expected peak width seem to be appropriate for the present data set. Evaluate
   the impact and result of this choice by applying the chromatographic peak
   detection on the EICs extracted in **Section 3.2**. The parameters for the
   *centWave* algorithm are configured with the `CentWaveParam()` function and
   peak detection is performed using `findChromPeaks()`. Note that a very low
   value for `snthresh` is used for peak detection in EIC signals because of the
   lack of background signal to properly estimate noise. Repeat this analysis
   for all EICs. Values for the `peakwidth` parameter should be adjusted if the
   results don't match the expectations.

```{r}
#' Define the peak detection settings
cwp <- CentWaveParam(peakwidth = c(2, 10),
                     snthresh = 1,
                     integrate = 2)

#' Perform peak detection for the proline EIC
proline_eic <- findChromPeaks(proline_eic, param = cwp)
```

2) Plot and visually inspect the chromatographic peak detection results of the
   EIC.

```{r}
#' Plot the proline EIC after peak detections
plot(proline_eic,
     peakCol = paste0(col_sample[chromPeaks(proline_eic)[, "column"]], 80),
     peakBg = paste0(col_sample[chromPeaks(proline_eic)[, "column"]], 40),
     col = col_sample, xlim = c(165, 175))
```

3) Calculate the average peak width in retention time dimension. This value
   across all EICs can be used to evaluate whether peak width match the
   expectation from the LC setup.

```{r}
#' Calculate the mean rt peak width
(chromPeaks(proline_eic)[, "rtmax"] -
 chromPeaks(proline_eic)[, "rtmin"]) |>
    mean()
```

4) Define the *centWave* `ppm` parameter. Using the tentative MS signal for
   proline from **Section 3.2.4** (i.e., the MS data from its approximate *m/z*
   and retention time window) the parts-per-million (ppm, *see* **Note 8**)
   deviation of the mass peaks' *m/z* values is calculated. This should be
   repeated for each of the ≥ 10 plotted EICs. Use the calculated values to
   define parameter `ppm`. Note that it is absolutely acceptable to use larger
   values for `ppm` than observed, in particular for data from time-of-flight
   (TOF) instruments where *m/z* deviation generally increases with lower
   intensities. Thus, to ensure that chromatographic peak boundaries are
   correctly estimated, it can be beneficial to use higher than observed values
   for the parameter `ppm`. For the present data set we use `ppm = 5`.

```{r}
#' Estimate m/z deviation for proline
mz_diff <- spectra(ms_data_proline) |>
    mz() |>
    unlist() |>
    diff()

ppm <- max(mz_diff) / proline_mz * 10^6
print(paste0("The expected ppm for proline is: ", round(ppm, 3)))
```

5) Perform chromatographic peak detection with the defined settings on the whole
   data set. Set parameter `snthresh` depending on whether only high abundance,
   high quality peaks should be detected (i.e., use a value >= 10), or also low
   abundance peaks, accepting eventually false positive peaks. For `snthresh`,
   we use a value of `5` (instead of the default `10`).

```{r}
#' Define the peak detection settings
cwp <- CentWaveParam(peakwidth = c(2, 10),
                     ppm = 5,
                     snthresh = 5,
                     integrate = 2)

#' Perform peak detection on the full data
ms_data <- findChromPeaks(ms_data, param = cwp, chunkSize = CORES)
```

6) Perform peak refinement (optional, but suggested for *centWave*). This step
   can help correct peak detection artifacts of the *centWave* algorithm. It
   should however be used with caution, as it may also introduce artificial
   corrections if not properly evaluated. By merging neighboring peaks, closely
   eluting or overlapping peaks that likely originate from the same compound are
   combined. This helps reduce redundancy and improves the accuracy of
   downstream analyses. Parameter `expandRt` should be set to a value close to,
   or smaller than, half of the average observed retention time width of
   chromatographic peaks and for `ppm` the same value as used for *centWave*
   peak detection can be used.

```{r}
#' Define the peak refinement settings
mnpp <- MergeNeighboringPeaksParam(expandRt = 3,
                                   ppm = 5)

#' Perform peak refinement
ms_data <- refineChromPeaks(ms_data, param = mnpp, chunkSize = CORES)
```

7) Summaries of identified peaks: determine the number of identified peaks per
   sample. Especially pay attention to the number
   of peaks per sample, and to the quantiles of retention time widths and *m/z*
   widths.

```{r}
#' View summary of identified peaks
data.frame(sample_name = sampleData(ms_data)$sample_name,
           peak_count = as.integer(table(chromPeaks(ms_data)[, "sample"]))) |>
  t() |>
  kable(format = "pipe")
```

8) Summaries of identified peaks: evaluate the *m/z* widths of identified
   chromatographic peaks. Calculate the distribution (quantiles) of the
   chromatographic peak widths in *m/z* dimension.

```{r}
#' Distribution of m/z peak widths
(chromPeaks(ms_data)[, "mzmax"] -
 chromPeaks(ms_data)[, "mzmin"]) |>
    quantile()
```

9) Summaries of identified peaks: evaluate the retention time widths of
   identified chromatographic peaks. Calculate the distribution (quantiles) of
   the chromatographic peak widths in retention time dimension.

```{r}
#' Distribution of retention time peak widths
(chromPeaks(ms_data)[, "rtmax"] -
 chromPeaks(ms_data)[, "rtmin"]) |>
    quantile()
```

### 3.3.2 Retention time alignment

Retention time alignment aims at adjusting the data for retention times shifts
between measurement runs (i.e., samples) [@smith_lc-ms_2013]. We use the *peak
groups* method from *xcms*. This algorithm performs a robust, non-linear
alignment based on retention times of signals, the so called *anchor peaks* that
are present in most samples. This requires an initial, loose, grouping of
chromatographic peaks across samples. This grouping of chromatographic peaks to
features does not need to be perfect yet, i.e., the signal from different ions
may be grouped into the same feature, as retention time alignment will correct
for systematic shifts. Grouping collects peaks that represent the same compound
across samples, even if retention times differ slightly. The *peak groups*
alignment method can then select anchor peaks among these initial LC-MS features
and align the samples by minimizing the retention time differences between the
anchor peaks.

1) Perform an initial correspondence analysis using the *peak density*
   method. Define the sample groups of the experiment and exclude samples that
   should not be considered for peak grouping/alignment by assigning `NA`. In
   the present example LC-MS/MS measurements are excluded. Set parameters
   `minFraction` depending on the experimental setting. We use `minFraction =
   2/3`, thus, to define a feature, a chromatographic peak has to be present in
   at least two out of the 3 samples per sample group. Parameters `binSize` and
   `ppm` depend on the resolution of the MS instrumentation and are calculated
   under point 4) and point 8) in **Section 3.3.1**
   respectively. Chromatographic peaks (within and between samples) with a
   difference in *m/z* value smaller than `binSize` + `ppm` will be grouped into
   a feature. The value for parameter `bw` depends on the observed difference of
   retention times between isomers as well as retention time shifts between
   samples. For this initial correspondence analysis it should be set to a
   larger value to allow/account for retention time differences between
   samples. A value close to or larger than the average observed retention time
   width of detected chromatographic peaks could be a good initial choice.

```{r}
#' Define sample groups and set samples that should not be
#' considered to `NA`
grp <- sampleData(ms_data)$concentration
grp[sampleData(ms_data)$mode == "DDA"] <- NA

#' Define the settings for the initial correspondence
pdp <- PeakDensityParam(sampleGroups = grp,
                        minFraction = 2 / 3,
                        binSize = 0.02,
                        ppm = 10,
                        bw = 4)

#' Perform the initial correspondence analysis
ms_data <- groupChromPeaks(ms_data, param = pdp)
```

2) Perform retention time alignment to correct systematic shifts between samples
   using the *peak groups* method. Configure the algorithm using
   `PeakGroupsParam()`. Parameters: `minFraction`: minimum fraction of samples
   that must share an anchor peak for alignment. This is relative to the total
   number of samples, not the number of samples per sample group (as
   above). Choices around 0.8 (presence of an anchor peak in 80% of samples)
   work for most experiments. We us a value of 0.9 as the same sample was
   repeatedly measured. `extraPeaks`: number of extra peaks per feature
   considered for robust alignment (optimally use a value similar to the number
   of samples in the experiment). `span`: controls the smoothness of the
   alignment curve. Must be a value between 0 and 1, smaller values fit more
   local variation, a value of 1 would perform a constant shift. A value between
   0.4 and 0.7 works in most cases. Parameter `subset` allows to configure an
   alignment that is based only on a subset of samples (such as e.g., quality
   control (QC) samples). Set `subset` to the indices of such QC samples or to
   the indices of the samples on which the initial correspondence was performed.

```{r}
#' Define retention time alignment parameters
pgp <- PeakGroupsParam(minFraction = 0.9,
                       extraPeaks = length(ms_data),
                       span = 0.4,
                       subset = which(!is.na(grp)))

#' Perform retention time alignment
ms_data <- adjustRtime(ms_data, param = pgp)
```

3) Evaluate alignment result. Plot the adjusted retention times (x-axis) against
   the difference between raw and adjusted retention times (y-axis). Deviations
   observed on they y-axis should meet the expectations from the variance
   generally observed on the employed LC setup. Data points (i.e., anchor peaks)
   should span most of the retention time range. If needed, adapt parameters and
   repeat the initial correspondence and alignment.

```{r}
#' Plot the adjusted rt vs. rt differences before and after correction
plotAdjustedRtime(ms_data, col = paste0(col_sample, 80))
grid()
```

4) Create the BPC after retention time alignment to visually compare with the
   original BPC.

```{r}
#' Create the BPC after rt alignment
bpc_adj <- chromatogram(ms_data, chromPeaks = "none", aggregationFun = "max", 
                        chunkSize = CORES)

#' Plot before and after rt alignment
par(mfrow = c(2, 1))
plot(bpc, col = paste0(col_sample, 60))
grid()
plot(bpc_adj, col = paste0(col_sample, 60))
grid()
```

5) Evaluate improved retention time alignment on each EICs, to visually before
   vs. after correction.

```{r}
#' Extract the EIC for proline with corrected rt
proline_eic_adj <- chromatogram(ms_data, mz = proline_mzr, chunkSize  = CORES)

#' Plot before and after rt alignment for proline EIC
par(mfrow = c(2, 1))
plot(proline_eic[1, ], col = paste0(col_sample, 80),
     peakType = "none", xlim = c(165, 175))
plot(proline_eic_adj[1, ], col = paste0(col_sample, 80),
     peakType = "none", xlim = c(165, 175))
```

6) Apply the alignment results to the data set.

```{r}
#' Apply the corrected rt to data set
ms_data <- applyAdjustedRtime(ms_data)
```

### 3.3.3 Correspondence analysis

Correspondence analysis groups signals (chromatographic peaks) across samples
that are presumably from the same original ions (compounds). This grouping
depends on similarity of the peaks' *m/z* and retention time values.

1) Derive settings for correspondence analysis. Extract the signal trace for the
   *m/z* of EICs defined in **Section 3.2.3**. Define parameters for the
   correspondence: values for `binSize` and `ppm` can be adopted from the
   initial correspondence analysis from **Section 3.3.2**. Parameter
   `minFraction` should be adapted to match the experimental setup. For `bw`,
   use the same value, or a smaller value, than used for the initial
   correspondence. Plot the resulting EIC and simulate a correspondence analysis
   with the defined configuration.

```{r}
#' Extract EIC for the full rt range for EICs' m/z ranges
eic <- chromatogram(ms_data, mz = proline_mzr)

#' Configure correspondence analysis
pdp <- PeakDensityParam(sampleGroups = grp,
                        minFraction = 2 / 3,
                        binSize = 0.02,
                        ppm = 10,
                        bw = 2)

#' Plot EIC and simulate correspondence
plotChromPeakDensity(eic, param = pdp, col = paste0(col_sample, 80))
grid()
```

2) Evaluate the resulting plot. Are chromatographic peaks grouped properly into
   features (chromatographic peaks, respectively the retention time of their
   apex, grouped into the same feature are indicated with grey rectangles in the
   lower part of the plot)? Are co- or closely eluting compounds separated into
   distinct features? Evaluate the results on all EICs defined in **Section
   3.2.3**. Eventually change one or more parameters of `PeakDensityParam()` and
   repeat until results are satisfactory.

3) Perform the correspondence analysis on the full data set.

```{r}
#' Final correspondence
ms_data <- groupChromPeaks(ms_data, param = pdp)
```

4) Evaluate results on EICs. Extract ion traces for the *m/z* ranges of EICs
   from **Section 3.2.3** and plot the correspondence results using
   `plotChromPeakDensity()`. Use parameter `simulate = FALSE` to show the actual
   correspondence results. Repeat for all *m/z* ranges from **Section 3.2.3**.

```{r}
#' Extract EIC for the full rt range for EICs' m/z ranges
eic <- chromatogram(ms_data, mz = proline_mzr)

#' Plot EIC and simulate correspondence
plotChromPeakDensity(eic, col = paste0(col_sample, 80),
                     simulate = FALSE)
grid()
```

5) Visualize the features's *m/z* and retention times in the full *m/z* and
   retention time space of the data set. To avoid loading the *m/z* values of
   all mass peaks we manually define the *m/z* range based on the MS instrument
   setup. Use `ylim = range(unlist(mz(spectra(ms_data))))` to calculate it from
   the data set.

```{r}
#' Plot m/z vs. rt for the data set
plot(NA, NA, xlim = range(rtime(spectra(ms_data))),
     ylim = c(0, 1000),
     xlab = "retention time", ylab = "m/z")
grid()
points(featureDefinitions(ms_data)$rtmed,
       featureDefinitions(ms_data)$mzmed,
       pch = 21, col = "#00000060", bg = "#00000020")
```

### 3.3.4 Gap filling

The feature abundance matrix can contain a considerable number of missing values
depending on whether for a particular feature a chromatographic peak was
detected in a sample or not as in the feature matrix abundance values for
identified chromatographic peaks are reported. The gap filling step aims at
reducing the number of missing values by integrating all raw MS signal from the
feature's expected *m/z* and retention time window in samples in which no
chromatographic peak was detected.

1) Determine the percentage of missing values in the feature abundance matrix.

```{r}
#' Get the total number, and the number of missing values
total_feat <- length(featureValues(ms_data))
miss_feat <- sum(is.na(featureValues(ms_data)))

#' Calculate the percentage of missing values in the data set
miss_feat / total_feat * 100
```

2) Perform the gap filling.

```{r}
#' Perform gap filling
ms_data <- fillChromPeaks(ms_data, ChromPeakAreaParam(),
                          chunkSize = CORES)
```

3) Determine the number of missing values after gap filling.

```{r}
#' Get the number of missing values after gap filling
miss_feat <- sum(is.na(featureValues(ms_data)))

#' Calculate the percentage of missing values in the data set
miss_feat / total_feat * 100
```

## 3.4 Post processing and data export

Following the preprocessing, the resulting feature table represents the
foundation for downstream statistical analysis and biological
interpretation. This post-processing phase typically involves several key steps
to refine and analyze the data further. These include normalization of feature
intensities across samples to correct for systematic variation, removal of
outlier features that may represent technical artifacts, and assessment of
sample variance to identify trends or inconsistencies.

Subsequently, differential abundance analysis is performed to identify metabolic
features that distinguish phenotypes or experimental conditions of
interest. Multivariate statistical methods, such as principal component analysis
(PCA), hierarchical clustering, (orthogonal) partial least squares discriminant
analysis ((O)PLS-DA), linear regression-based approaches, or machine learning
algorithms are often employed to visualize global patterns and highlight
relevant features.

Finally, metabolite annotation of significant or interesting features is a
critical step for biological interpretation, enabling insights into the
metabolic pathways or processes involved. The exact post-processing pipeline may
vary depending on the research question, experimental design, and biological
matrix.

For practical code examples and guidance on the post processing, we refer the
reader to the many [example
workflows](https://rformassspectrometry.github.io/Metabonaut/index.html) of the
Metabonaut resource [@louail_metabonaut_2025], which provides detailed and
modular R-based workflows for each component of post-processing.

### 3.4.1 Data export

After completing preprocessing, and optionally some further data filtering
and/or normalization, the feature table can be exported for downstream analysis,
sharing, or archiving. This step ensures reproducibility and allows
interoperability with other software tools for statistical analysis or
metabolite identification. Below, we outline key procedures to export the
processed data, including both the feature intensity matrix and the features'
MS/MS spectra.

1) Export feature table as a `SummarizedExperiment` object (*see* **Note
   9**). Parameter `filled = FALSE` extracts only abundances of detected
   chromatographic peaks, but no gap filled values.

```{r}
#' Extract feature intensity results as a SummarizedExperiment
res <- quantify(ms_data, method = "sum", filled = FALSE)

#' View the summary of the result object
res
```

2) Add a second assay containing missing-value filled data to the
   `SummarizedExperiment` object. The additional assay contains filled intensity
   values (i.e., using gap-filling methods for missing peaks), so it can be used
   in downstream imputation-aware analysis.

```{r}
#' Add a second assay named 'raw_filled' with gap-filled intensities
assays(res)$raw_filled <- featureValues(ms_data, method = "sum",
                                        filled = TRUE )

#' View different assays in the SummarizedExperiment object
assayNames(res)

#' View the top of the filled feature intensity matrix
assay(res, "raw_filled") |> head()
```

3) Export the feature abundance matrix to a spreadsheet. The ID of the features
   and their *m/z* and retention time are added as additional columns to this
   feature abundance matrix. For ease of data sharing or as input into external
   tools (e.g., Excel, Python), the feature intensity matrix can be exported as
   a .csv file.

```{r}
#' Compile a data.frame with feature definitions and abundances
fd_data <- data.frame(
    feature_id = rownames(rowData(res)),
    rowData(res)[, c("mzmed", "rtmed")],
    assay(res, "raw_filled")
)

#' Write the filled feature intensity matrix to a CSV file
write.csv(fd_data,
          file = "data/standards_abundance.csv")
```

4) Export the MS/MS spectra in MGF format. To support metabolite annotation in
   external tools (e.g., GNPS, MS-DIAL, or SIRIUS), the MS/MS spectra associated
   with detected MS features are exported to the Mascot Generic Format
   (MGF). The `MsBackendMgf()` backend function is used here to write the
   compatible files. To allow association between the exported feature
   abundances and the MS/MS spectra, the MGF file contains a field *feature_id*
   containing the identifier of the feature for each MS/MS spectrum.

```{r}
#' Extract the MS/MS spectra for each feature
sp <- featureSpectra(ms_data)

#' Export the MS/MS spectra in .mgf format
export(sp, MsBackendMgf(), file = "data/std_spectra.mgf")
```

## 3.5 Session information

1) Print the R session info in the rendered Quarto document (*see* **Note 10**).

```{r}
#' View R session info
sessionInfo()
```

# Notes

1) Writing workflow processes using Quarto documents (.qmd) supports literate
   programming [@knuth_literate_1984] by integrating code, text, and outputs in
   a single file. Quarto enables the creation of fully reproducible documents by
   embedding R code directly within narrative text, ensuring that results are
   dynamically generated from the most up-to-date data. This approach
   facilitates transparent, data-specific analysis workflows that are easy to
   audit, share, and update. By combining documentation and computation, Quarto
   helps streamline reporting, improve collaboration, and enhance the
   reproducibility of scientific and statistical analyses.

2) The workflow starts from vendor-neutral raw MS files, already converted from
   vendor-specific proprietary raw file formats (e.g., Thermo Fisher, Agilent,
   Bruker, Waters) to the open standard for MS data, i.e. the mzML format.
   Conversion of vendor-specific raw MS files is performed using *MSConvert*, a
   powerful and flexible tool from *ProteoWizard*. Using mzML as a standard
   format ensures that your MS data can be analyzed in a reproducible,
   tool-independent way.

3) R packages can be installed from the Comprehensive R Archive Network (CRAN)
   or from Bioconductor using the following commands:

```
#' Install CRAN packages
install.packages("knitr")

#' Install Bioconductor packages
install.packages("BiocManager")
BiocManager::install("BiocStyle")
```

4) Alternatively to using the `read_xlsx()` function from the *readxl* R
   package, the same tabular information could also be defined directly in R as
   a `data.frame` object, but it is generally better, and more convenient to the
   user, to define and load such spreadsheet from as spreadsheet program.

5) For improved visualization, it is recommended to assign specific colors to
   individual samples as well as to sample groups. These color settings can be
   re-used in all plots during the analysis. Suggested color-coding schemes
   include: acquisition batch, injection order, sample type (e.g., QC, spiked,
   biological), and biological phenotypes of interest (e.g., sex, case-control
   status). These visual cues aid in assessing data quality and identifying
   potential systematic variation.

6) When no prior information is available about expected compounds, select
   high-intensity peaks from the BPC, extract full MS1 scans, and evaluate EICs
   to derive retention time and *m/z* parameters empirically.

7) Accurate peak detection requires centroided data and a prerequisite for the
   chromatographic peak detection algorithm. If your data is not centroided, use
   MSConvert or a similar tool to convert profile mode data to centroid mode
   before importing it into R.

8) The *m/z* mass deviation is expressed as ppm and not in *m/z* difference (in
   Da) because the variation scales with the molecular mass of the compound. It
   is important to estimate the ppm also on large compounds, as the ppm
   increases with larger compounds. For example, if your highest *m/z* detected
   is 1000 Da, and you have an estimated error of 5 ppm, the largest possible
   *m/z* deviation allowed is 1000 ± 0.005 Da (*m/z* mass deviation = 1000 Da *
   5 ppm / 10^6), ranging from 999.995 to 1000.005 Da.

9) The `SummarizedExperiment` container is a robust Bioconductor class that
   holds both assay data (e.g., intensity values) and associated metadata (e.g.,
   feature and sample annotations in it's `rowData()` and `colData()`,
   respectively). It provides a standardized format for integrating omics data
   and simplifies downstream manipulation or analysis.

10) Including a call to sessionInfo() at the end of the workflow is important
    for ensuring reproducibility, as it documents the versions of R and all
    loaded packages, as well as system-specific details that may influence
    results.

# References
