---
title: "LC-MS data processing using *xcms*"
format:
  html:
    toc: true
    self-contained: true
author: "Marilyn De Graeve, Philippine Louail, Johannes Rainer"
code_folding: hide
editor: source
date: 'Compiled: `r format(Sys.Date(), "%B %d, %Y")`'
bibliography: lcms-data-preprocessing.bib
#csl: springerprotocols.csl #FINDME @mdg: temp, works but not on cluster
---

```{r setup, include=FALSE}
library(knitr)
library(BiocStyle)
knitr::opts_knit$set(root.dir = './')
```

# Abstract

*#FINDME 200 words. open access, teaser. (below one suggestion by GTP) @jo: ok?**

Liquid chromatography-mass spectrometry (LC-MS) is a cornerstone technology in
metabolomics, enabling high-throughput detection of small molecules across
diverse biological samples. However, raw LC-MS data are complex, requiring
careful preprocessing to ensure accurate and reproducible feature
detection. This chapter introduces a step-by-step protocol for LC-MS data
preprocessing using the open-source *xcms* package in R. Designed for users
ranging from beginners to experienced analysts, the chapter outlines critical
stages including data inspection, peak detection, retention time alignment,
feature grouping, and data export. Special attention is given to parameter
optimization and diagnostic visualization to guide users in making informed
decisions tailored to their specific dataset. We demonstrate the approach on
human serum samples, using real-life examples such as proline to showcase
retention time alignment and peak correspondence. By combining practical code
snippets with conceptual insights, this chapter empowers researchers to harness
*xcms* for robust, reproducible LC-MS workflows in untargeted
metabolomics. Whether you are setting up your first analysis or refining an
established pipeline, this chapter serves as both a tutorial and a reference for
high-quality LC-MS data processing.


# Key words

*#FINDME 5-10 #. @jo: ok?*

*xcms*, Liquid chromatography, Mass spectrometry, Metabolomics, Data processing,
Preprocessing, Peak detection, Retention time correction, Quality control

# 1. Introduction

Liquid chromatography-mass spectrometry (LC-MS) has become a cornerstone
technique in metabolomics, proteomics, and other omics disciplines due to its
high sensitivity, selectivity, and ability to detect a broad range of compounds
in complex biological samples. However, the raw data generated by LC-MS
instruments is complex and multidimensional, requiring careful preprocessing to
extract meaningful and reproducible quantitative information. Key steps of the
preprocessing typically include chromatographic peak detection, retention time
alignment, correspondence analysis, and gap filling.

This chapter introduces the *xcms* R package, a widely adopted, open-source tool
designed for LC-MS data preprocessing and analysis.  Developed with a strong
foundation in statistical computing and reproducible research principles, *xcms*
provides robust and flexible algorithms for key tasks such as peak detection and
retention time correction and feature alignment across samples. Its seamless
integration with the broader R ecosystem allows for the entire workflow, from
initial inspection to normalization and downstream statisitcal analysis, to be
executed within a single, reproducable envrinment. Moreover, *xcms* is optimized
to handle computationally intensive, big datasets, making it suitable for both
exploratory and large-scale studies.

In this chapter, we will demonstrate a complete, reproducible LC-MS data
preprocessing workflow using *xcms*, illustrated with a small untargeted LC-MS
metabolomics dataset. Special emphasis is placed on the best practices for data
handling, parameter optimization, and quality control. By the end of this
chapter, readers will have a strong foundation in using *xcms* to transform raw
LC-MS data into structured high-quality feature tables suitable for downstream
statistical analysis and biological interpretation.

*#FINDME @jo/phili: want to add maybe some news from the net xcms preprint to
the into too?*

# 2. Material

This provides an overview of the dataset, software environment, and workflow
used for LC-MS(/MS) data preprocessing with the *xcms* R package. The entire
analysis is designed to be reproducible and made publicly available.


## 2.1 Overview of the workflow

To make the data processing steps transparent and reproducible, the analysis is
implemented as a Quarto document (.qmd). Quarto enables seamless integration of
code, narrative, and output, allowing readers to execute and modify the workflow
directly (*see* **Note 1**). While any code editor may be used, RStudio offers
built-in support for both R and Quarto and is a commonly used interface in data
science.

All code and data required to follow this tutorial are available at:
https://github.com/jorainer/xcms_preprocessing_chapter. To try out the protocol,
users should first download and unzip the provided dataset (see below) into a
directory named MTBLSXX within their R working directory.


## 2.2 Dataset

The LC-MS data used in this chapter comes from a small, publicly available
untargeted metabolomics study. Samples were analyzed using HILIC UPLC-QTOF-MS in
both positive and negative ionization modes, as described in *XXX [FINDME @jo:
REF to paper LC-MS METHOD]*. Raw data files are provided in the open and
vendor-independent .mzML format (*see* **Note 2**).

The dataset consists of pooled human serum samples, which represent internal
quality control (QC) samples, each spiked with known compounds at two
concentration levels: high and low. Each sample was measured in the positive
ionisation mode and as a technical triplicate to assess reproducibility
(Table 1).

Additionally, two LC-MS/MS runs were acquired for one of the high concentration
samples using data-dependent acquisition (DDA) at two different collision
energies (20 eV and 30 eV). These are included to support downstream MS/MS-based
compound annotation (Table 1).

The dataset is publicly available on MetaboLights (accession: *MTBLSXX [FINDME
@jo/phili: deposit and add name]*) and can be downloaded from the archive for
use in this tutorial.

**Table 1**. Experimental design of the small untargeted metabolomics dataset. 

| mzML	| polarity	| time	| mode	| collision_energy	| concentration	| sample_name | sample_origin | sample_type |
|:-----:|:---------:|:-----:|:-----:|:-----------------:|:-------------:|:-----------:|:-------------:|:-----------:|
| QC_LowIS_Mix13_1_POS.mzML	| POS	| 2020-01-17 14:33:51	| MS1	| 	| low	| low_a | serum	| pool QC |
| QC_HighIS_Mix13_3_POS.mzML	| POS	| 2020-01-17 14:51:21	| MS1	| 	| high	| high_c | serum	| pool QC |
| QC_LowIS_Mix13_3_POS.mzML	| POS	| 2020-01-17 15:46:56	| MS1	| 	| low	| low_c | serum	| pool QC |
| QC_HighIS_Mix13_2_POS.mzML	| POS	| 2020-01-17 17:58:34	| MS1	| 	| high	| high_b | serum	| pool QC |
| QC_LowIS_Mix13_2_POS.mzML	| POS	| 2020-01-17 20:48:23	| MS1	| 	| low	| low_b | serum	| pool QC |
| QC_HighIS_Mix13_1_POS.mzML	| POS	| 2020-01-17 22:48:18	| MS1	| 	| high	| high_a | serum	| pool QC |
| QC_HighIS_Mix13_CE20_POS.mzML	| POS	| 2020-01-18 19:27:09	| DDA	| 20	| high	| high_20 | serum	| pool QC |
| QC_HighIS_Mix13_CE30_POS.mzML	| POS	| 2020-01-18 19:39:02	| DDA	| 30	| high	| high_30 | serum	| pool QC |


## 2.3 Software and R packages

The data analysis was performed using R (version ≥ 4.5.0), a statistical
programming language available for Windows and Unix (Linux, and macOS) systems.

The core LC-MS(/MS) preprocessing workflow is built upon the following R
packages:

- *Spectra* is used to import and handle the raw mass spectrometry data,
     providing efficient and flexible access to spectral information
     [@rainer_modular_2022].

- *MsExperiment* is employed to manage the experimental design and to link
    sample metadata with the corresponding raw data files
    [@rainer_modular_2022]. *#FINDME @jo: correct ref or another?*

- *xcms* performs key preprocessing steps including chromatographic peak
     detection, retention time alignment, and feature grouping
     [@temp_xcms_preprint].

Complementary packages that enhance and support mainly metabolomics-specific
analyses include:

- *MetaboCoreUtils* offers core utility functions for metabolomics data
   processing, including mass-to-charge (*m/z*) and retention time calculations,
   adduct annotation, and common data transformations used across multiple
   packages [@rainer_modular_2022]. *#FINDME @jo: correct ref or another?*

- *SummarizedExperiment* provides a standardized container for storing assay
    data (e.g., feature intensities) alongside row (feature) and column (sample)
    metadata, facilitating consistent data handling and integration in
    metabolomics workflows [@rainer_modular_2022]. *#FINDME @jo: correct ref or
    another?*

- *MsBackendMgf* implements support for reading and writing mass spectral data
   in the Mascot Generic Format (MGF), commonly used for storing MS/MS spectra
   in metabolomics and proteomics annotation tasks
   [@rainer_modular_2022]. *#FINDME @jo: correct ref or another?*

Additional R packages support the workflow by enabling data visualization
(*RColorBrewer*, *pheatmap*), reading of metadata and tabular input files
(*readxl*), and providing utility functions that enhance reproducibility and
automation (*knitr*, *BiocStyle*).


# 3. Methods

## 3.1 Data import

In this section, general setup steps are performed, including loading the
required R packages, configuring parallel processing, and specifying the data
directory. The sample metadata and raw LC-MS data are then loaded and
color-coded to facilitate clear and consistent visualization throughout the
analysis.

1) Load all required R/Bioconductor packages for the present analysis (*see*
**Note 3**).

```{r}
#| message: false

#' Load all R packages
library(xcms)
library(MsExperiment)
library(Spectra)
library(readxl)
library(RColorBrewer)
library(pheatmap)
library(MetaboCoreUtils)
library(SummarizedExperiment)
library(MsBackendMgf)
```

2) Configure parallel processing. Configure the number of cores, using the
   `CORES` variable. All data analysis functions in *xcms* have built-in
   parallel processing capabilities. In addition, to reduce memory load, *xcms*
   performs analyses in a *chunk-wise manner*, i.e. only data from a certain
   number of samples is loaded and processed at a time. This can be configured
   with the parameter `chunkSize = ` in most data analysis
   functions. Importantly, the number of CPU cores used should be smaller or
   equal to the value for this parameter. Below we chose to use 2 CPU cores for
   parallel processing. R supports on Unix systems the *multi-core* parallel
   processing, while on Windows systems *simple network of workstation*
   (SNOW)-based parallel processing must be used.

```{r}
#' Define the number of cores
CORES <- 2
CORES <- 7 #FINDME temp @mdg: only for run on cluster

#' Set up parallel processing using 2 cores
if (.Platform$OS.type == "unix") {
    register(MulticoreParam(CORES))
} else {
    register(SnowParam(CORES))
}
```

3) Load experimental setup/sample information. It is good practice to define the
   experimental setup using e.g. a spreadsheet that contains the names of all
   of the experiment's data files (eventually including their path) and their
   respective samples including all phenotypical information as well as
   technical/experimental properties. Ideally, the order of the samples should
   match their injection sequence. The content of the spreadsheet for the
   present experiment is shown in Table 1. Load the experiment's spreadsheet
   using the `read_xlsx()` function from the *readxl* R package (*see*
   **Note 4**).

```{r}
#' Import sample file
sample_data <- read_xlsx("data/standards_mzml.xlsx") |> as.data.frame()
```

4) Create a *data representation* for the experiment. The `readMsExperiment()`
   function takes the names of the data files (including the path to the
   directory where they are stored, configured below using the `MZML_PATH`
   variable) and a `data.frame` with the sample information for the respective
   data files as input. The resulting `MsExperiment` object (variable
   `sample_data`) contains a reference to the MS data files and the sample
   information.

```{r}
#' Define the path to the directory where the data files can be found
MZML_PATH <- file.path("data/mzML/")
MZML_PATH <- "/data/massspec/mzML/2020/2020_01" #FINDME temp @mdg: only for run on cluster

#' Linking the raw data to its respective metadata.
std_data <- readMsExperiment(file.path(MZML_PATH, sample_data$mzML),   #FINDME @jo/phili: good name or change?
                             sampleData = sample_data)
```

5) Check if the *data representation* is correct by printing the sample
   information table. Sample information can be extracted using the
   `sampleData()` function.

```{r}
#| tbl-cap: "Data files and samples."

#' View the sample information table
sampleData(std_data)[, c("mzML", "sample_name", "mode", "collision_energy",
                         "concentration")] |>
    kable(format = "pipe")
```

6) Define colors to label individual or groups of samples (*see* **Note 5**).

```{r}
#' Define colors by concentration levels: high and low
col_concentration <- c(low = "#FF7F00", high = "#A65628")
col_sample <- col_concentration[sampleData(std_data)$concentration]
```


## 3.2 Initial data inspection

Before initiating the preprocessing of LC-MS data, it is essential to evaluate
the overall data quality and define key parameters. In this section, we perform
the visual inspection of the chromatographic base peak and extracted-ion
chromatograms (EICs), enabling the identification of poorly acquired samples,
retention time regions with meaningful signal, and potential contaminants. We
also verify whether the data was acquired in centroid mode, a requirement for
peak detection, and extract signal characteristics such as chromatographic peak
width and *m/z* mass deviation. These initial steps help ensure that the data is
suitable for automated processing and allow optimization of parameter settings
for robust peak detection and alignment.


### 3.2.1 Visualize base peak chromatogram

1) Create a base peak chromatogram (BPC) of the full dataset and plot it. The
   BPC is plotted to assess chromatographic drift or shifts, sample quality or
   outliers, and the presence of systematic noise or chemical contamination
   (e.g., polymer peaks). With parameter `aggregationFun = "max"` the highest
   intensity per spectrum is reported, while setting `aggregationFun = "sum"`
   would sum-up all intensities per spectrum, hence generating a total ion
   chromatogram (TIC).

```{r}
#' Create a BPC
bpc <- chromatogram(std_data, aggregationFun = "max", chunkSize = CORES)

#' Plot the BPC
plot(bpc, col = paste0(col_sample, 80))
grid()
abline(v = c(15, 225), lty = 2)
legend("topright", col = col_concentration, lty = 1,
       legend = names(col_concentration))
```


### 3.2.2 Retention time filtering

2) To focus analysis on the region where meaningful compound separation and
   detection occur, filter the data to a specific retention time window, e.g.,
   between 15 and 225 seconds. This can be guided by known elution times of
   spiked or expected endogenous compounds.

```{r}
#' Filter retention time
std_data <- filterSpectra(std_data, filterRt, c(15, 225))
```

3) Create a BPC of the rt filtered dataset for further evaluation (*see*
   **Section 3.3.2**).

```{r}
#' Extract the BPC again - for reference later.
bpc <- chromatogram(std_data, aggregationFun = "max")
```


### 3.2.3 Extracted-ion chromatogram selection

4) Determine the *m/z* range for proline. Starting from its chemical formula,
   calculate the theoretical mass and subsequently the theoretical *m/z* for one
   of its ions. We use `"[M+H]+"` as the most likely adduct of proline in
   positive polarity, but could also specify any other possible adduct, such as
   `"[M+Na]+`, `"[M+NH4]+` etc. Create a range around the theoretical *m/z* by
   subtracting and adding a value of 0.01 Da. Depending on the accuracy and
   precision of the MS instrument, this value might be increased or decreased.

```{r}
#' Calculate the mass of proline
proline_formula <- "C5H9NO2"
proline_mass <- calculateMass(proline_formula)

#' Calculate m/z for an adduct of proline
proline_mz <- mass2mz(proline_mass, c("[M+H]+"))[1, ]
proline_mzr <- cbind(mzmin = proline_mz - 0.01, mzmax = proline_mz + 0.01)
```

5) Extract and plot the EIC of proline. 

```{r}
#' Extract the EIC for proline
proline_eic <- chromatogram(std_data, mz = proline_mzr, rt = c(160, 200))

#' Plot the EIC for proline
plot(proline_eic, col = paste0(col_sample, 80), xlim = c(165, 175))
```

6) Repeat this for minimum 10 compounds and save the EIC objects. Inspect
   molecules that you have spicked (e.g., internal standards), are known to be
   endogenously present (e.g., amino acids in serum), or select unknown
   compounds (*see* **Note 6**). Be aware to select compouds with retention time
   and *m/z* values representing the full chromatogram and spectra range,
   respectively. If the expected retention time range is unknown, omit the
   parameter `rt` of the `chromatogram()` function and extract the ion trace for
   the full full retention time range.

```{r}
#' Extract the EIC for an unknown compound
unknown_eic <- chromatogram(std_data, mz = c(106.0399, 106.0599))

#' Plot the full EIC
plot(unknown_eic, col = paste0(col_sample, 80))
```


### 3.2.4 Check *m/z* data acquisition mode

7) Evaluate whether data is acquired in centroid mode in the first sample, using
   proline (*see* **Note 7**). In addition, determine the variance in peaks'
   *m/z* along the retention time axis for individual ions.

```{r}
std_data_proline <- std_data[1] |>
    filterSpectra(filterRt, c(167, 172)) |>
    filterSpectra(filterMzRange, proline_mzr[1, ])

plot(std_data_proline)
```


### 3.2.5 Define preprocessing parameters 

*#FINDME @jo: OK, I arranged like this? not yet finished, see many notes...*

8) Inspect extracted chromatographic peaks of the ≥ 10 plotted EICs and define
   key variables to be used during the peak detection in the preprocessing. For
   the `peakwidth` setting, the rule of thumb is: between half of the observed
   peak width to maximum the double of peak width (in seconds). For the
   `snthresh` setting, the default value of "10" is lowered to "5" because the
   signal to ratio threshold is quite low (*see* EIC of proline). For the
   `integrate` setting, "2" is selected as the EICs show Gaussian
   chromatographic peak shapes (*see* EIC of proline).

```{r}
#' Estimate retention time peak width for proline 
#FINDME @phili/jo?: possible to replot but with dotted vertical lines and (at center and/or apix an "*", at left and right of half hight e dotted line green and at double with a red dotted line + text box on topright corner to have width in sec. so easy per EIC to see OR can you write good eg local min fuction to estimate this (on EIC with proline), would this be possile for all samples instead of one 1? OR no code chunk

#FINDME @jo: very hard to read y-axis of noise and top peak to choose snthreshold, maybe also with calculation? looks like 30000 / 1000 = 30 so looks fine to me?
```

9) Using the spectra (i.e. the *m/z* window), the `ppm` setting for peak
   detection, expected *m/z* mass deviation expressed in parts per million (ppm)
   (*see* **Note 8**) is calculated for each of the ≥ 10 plotted EICs.

```{r}
#' Estimate m/z deviation for proline #FINDME @phili/jo?: idem possible for all samples?
mz_diff <- spectra(std_data_proline) |>
    mz() |>
    unlist() |>
    diff()

ppm <- max(mz_diff) / proline_mz * 10^6
print(paste0("The expected ppm for proline is: ", round(ppm, 3)))
```

10) For the retention alignment and the correspondence analysis steps of the
    preprocessing, the estimated bandwidth for the peak density is
    defined. First, during intial correspondance, a wide bandwidth for smoothing
    is selected. As second bandwidth is defined, used during the correspondance
    analysis, which is more stringent and narrow.

```{r}
#' Estimate the bandwidth for the peak density for proline
#FINDME @phili/jo?: can you write fuction to estimate this (on EIC with proline), including +x sec for the broad overestimation needed in the RT alignment? and +y for narrow BW in the correspondance OR no code chunk. OR also figure with dotted lines?
#FINDME: @jo: question, is this also in seconds?
```

11) Save the user-defined preprocessing settings as `PEAKWIDTH`, `SNTHRESH`,
    `INTEGRATE`,`PPM`, `BW_WIDE`, and `BW_NARROW` variables, to be used in the
     consecutive steps of the preprocessing. 

```{r}
#' Define retention time peak width (in seconds)
PEAKWIDTH <- c(2, 8)

#' Define the signal-to-noise ratio threshold
SNTHRESH <- 5

#' Define the integrate setting
INTEGRATE <- 2

#' Define the m/z deviation (in ppm)
PPM <- 5 #FINDME: 40, 10, 20 @jo/phili: I am genually confused why you employ different ppms, we can explain the extra overestimation but why need to change and not use 5 ppm everywhere? this should be clear at this section. see previous version which ppm used where

#' Define the broad and narrow bandwidth
BW_WIDE <- 4
BW_NARROW <- 3
```


## 3.3 Data preprocessing

In this section, we outline the essential steps of LC-MS data preprocessing
using the *xcms* package. Chromatographic peak detection identifies relevant
features in the raw data based on intensity patterns across retention time and
mass-to-charge ratio. These features are then aligned across samples to correct
for systematic retention time shifts, ensuring accurate comparison.
Correspondence analysis groups peaks from different samples that represent the
same underlying compound, based on optimized density and alignment parameters.
Finally, gap filling recovers missing intensity values where peaks were not
initially detected, producing a complete and consistent feature table ready for
downstream statistical analysis.


### 3.3.1 Chromatographic peak detection

1) Derive the setting information for peak detection retained from the previous
   section. See the [xcms
   documentation](https://bioconductor.org/packages/xcms/) for further
   finetuning of the default preprocessing settings.  *#FINDME @jo: good link?*

```{r}
#' View the selected peak detection parameters
cat("The selected peak width is: ", PEAKWIDTH, "\n")
cat("The selected signal-to-noise ratio threshold is: ", SNTHRESH, "\n")
cat("The selected integrate setting is: ", INTEGRATE, "\n")
cat("The selected *m/z* deviation is: ", PPM, "\n")
```

2) Perform chromatographic peak detection on a selection of the data first,
   i.e., for each EIC to confirm accurate peak picking. The CentWave method was
   selected, called within the `findChromPeaks` function, as it works with most
   LC-setups. View the top of the retrieved peaks and plot the EIC upon peak
   detection for proline.

```{r}
#' Define the peak detection settings
cwp <- CentWaveParam(peakwidth = PEAKWIDTH,
                     ppm = PPM,
                     snthresh = SNTHRESH, 
                     integrate = INTEGRATE)

#' Perform peak detection for the proline EIC
proline_eic <- findChromPeaks(proline_eic, param = cwp)

#' View the top of the retrieved peaks
head(chromPeaks(proline_eic))

#' Plot the proline EIC after peak detections
plot(proline_eic,
     peakCol = paste0(col_sample[chromPeaks(proline_eic)[, "column"]], 80),
     peakBg = paste0(col_sample[chromPeaks(proline_eic)[, "column"]], 40),
     col = col_sample, xlim = c(165, 175))
```

3) Perform chromatographic peak detection on the whole dataset using paralel
   processing.

```{r}
#' Define the peak detection settings
cwp <- CentWaveParam(peakwidth = PEAKWIDTH,
                     ppm = PPM,
                     snthresh = SNTHRESH, 
                     integrate = INTEGRATE)

#' Perform peak detection on the full data
std_data <- findChromPeaks(std_data, param = cwp, chunkSize = CORES)
```

4) Perform peak refinement (optional). This step can help correct artifacts
   introduced during initial peak detection; however, it should be used with
   caution, as it may also introduce artificial corrections if not properly
   evaluated. By merging neighboring peaks, closely eluting or overlapping peaks
   that likely originate from the same compound are combined. This helps reduce
   redundancy and improves the accuracy of downstream analyses.

```{r}
#' Define the peak refinement settings
mnpp <- MergeNeighboringPeaksParam(expandRt = 3, 
                                   expandMz = 0.002, 
                                   ppm = 10,
                                   minProp = 0.8)

#' Perform peak refinement                           
std_data <- refineChromPeaks(std_data, param = mnpp, chunkSize = CORES)
```

5) View the summary of identified peaks. Especially pay attention to the number
   of peaks per sample, and to the quantiles of retention time widths and *m/z*
   widths.

```{r}
#' View summary of identified peaks
data.frame(sample_name = sampleData(std_data)$sample_name,
           peak_count = as.integer(table(chromPeaks(std_data)[, "sample"]))) |>
  t() |>
  kable(format = "pipe") #FINDME @phili: could we also add the quantiles of retention time widths or other interesting stats?
```


### 3.3.2 Retention time alignment

1) Derive the setting information for the retention time alignment retained from
   the previous section. See the [xcms
   documentation](https://bioconductor.org/packages/xcms/) for further
   finetuning of the default preprocessing settings.  *#FINDME @jo: good link?*

```{r}
#' View the selected retention time alignment parameters
cat("The selected bandwidth for the peak density is: ", BW_WIDE, "\n")
```

2) Define peak density parameters for the feature grouping on a selection of the
   data, i.e., for each EIC. Use the less stringent bandwidth for the peak
   density `BW_WIDE` to allow grouping across broader retention time
   windows. The remaining parameters were default: minimum fraction of samples
   that must have a peak (minFraction), and bin size for *m/z* binning. See the
   [xcms documentation](https://bioconductor.org/packages/xcms/) for further
   finetuning of the default preprocessing settings.  #*FINDME @jo: good link?*

```{r}
#' Group samples by concentration levels: high and low, and remove MS/MS data
grp <- sampleData(std_data)$concentration
grp[sampleData(std_data)$mode == "DDA"] <- NA

#' Define peak density parameters for grouping features
pdp <- PeakDensityParam(sampleGroups = grp,
                        bw = BW_WIDE,
                        minFraction = 2 / 3,
                        binSize = 0.02,
                        ppm = PPM)

#' Plot the EIC of proline using the defined PeakDensityParam
plotChromPeakDensity(proline_eic[1, ], param = pdp)
```

3) Perform the initial correspondence of chromatographic peaks into
   features. The grouping of chromatographic peaks to features does not need to
   be perfect yet, i.e. the signal from different ions may be grouped into the
   same feature, as retention time alignment will correct for systematic shifts.
   Grouping collects peaks that represent the same compound across samples, even
   if retention times differ slightly.

```{r}
#' Initial correspondence
std_data <- groupChromPeaks(std_data, param = pdp)
```

4) Perform retention time alignment to correct systematic shifts between samples
   using peak groups.  Use the `PeakGroupsParam()` rt alginment settings
   `minFraction`: minimum fraction of samples that must share an anchor peak for
   alignment, `extraPeaks`: number of extra peaks per feature considered for
   robust alignment (i.e., For a dataset with 8 samples, `extraPeaks = 1` uses
   all peak groups with a total peak count <= 8 + 1), and `span`: controls the
   smoothness of the alignment curve (smaller values fit more local variation).  

```{r}
#' Define retention time alignment parameters
pgp <- PeakGroupsParam(minFraction = 6 / 8, #FINDME @phili: why here different? how get this, I want to add to the initial inspection otherwise
                       extraPeaks = 5, #FINDME @phili: why initial so high as 50?
                       span = 0.4)

#' Perform retention time alignment
std_data <- adjustRtime(std_data, param = pgp, chunkSize = CORES)

#' Plot the difference in rt before vs. after rt alignment
plotAdjustedRtime(std_data)
```

5) Create the BPC after retention time alignment to visually compare with the
   original BPC.

```{r}
#' Create the BPC after rt alignment
bpc_adj <- chromatogram(std_data, chromPeaks = "none", aggregationFun = "max")

#' Plot before and after rt alignment
par(mfrow = c(2, 1))
plot(bpc, col = paste0(col_sample, 60))
grid()
plot(bpc_adj, col = paste0(col_sample, 60))
grid()
```

6) Evaluate improved rt alignment on each EICs, to visually before vs. after
   correction.

```{r}
#' Extract the EIC for proline with corrected rt
proline_eic_adj <- chromatogram(std_data, mz = proline_mzr)

#' Plot before and after rt alignment for proline EIC
par(mfrow = c(2, 1))
plot(proline_eic[1, ], col = paste0(col_sample, 80),
     peakType = "none", xlim = c(165, 175))
plot(proline_eic_adj[1, ], col = paste0(col_sample, 80),
     peakType = "none", xlim = c(165, 175))
```

7) Apply the corrected retention times (also in seconds) from the retention time
   alignment to the dataset.

```{r}
#' Apply the corrected rt to dataset
std_data <- applyAdjustedRtime(std_data)
```


### 3.3.3 Correspondence analysis

1) Derive the setting information for the retention time alignment retained from
   the previous section. 

```{r}
#' View the selected retention time alignment parameters
print(paste0(" The selected bandwidth for the peak density is: "), BW_NARROW)
```

2) Define peak density parameters for the feature grouping on a selection of the
   data, i.e., for each EIC. Use the more stringent bandwidth for the peak
   density `BW_NARROW`, as peaks belonging to the same feature are expected to
   be detected closer to each other after rt alignment. The setting `simulate =
   FALSE` visualizes real data peak density and helps you to assess if the
   chosen parameters capture the grouping behavior accurately.

```{r}
#' Define peak density parameters for grouping features
pdp <- PeakDensityParam(sampleGroups = grp,
                        bw = BW_NARROW,
                        minFraction = 2 / 3,
                        binSize = 0.02,
                        ppm = PPM)

#' xxxx comment to add, FINDME @phili
#' #proline_eic_adj <- chromatogram(std_data, mz = proline_mzr, rt =
#                                  cbind(rep(150, 3), rep(200, 3)))
#FINDME @phili: why need cbind? why not works with previous object proline_eic_adj OR one below?
#proline_eic_adj <- chromatogram(std_data, mz = proline_mzr, rt = c(160, 200))  

#' Plot the EIC of proline using the defined PeakDensityParam
#plotChromPeakDensity(proline_eic_adj[1, ], param = pdp,  simulate = FALSE) #FINDME @jo: why does this not plot inline? now error: Error in `!missing(fts) && nrow(fts)`: ! invalid 'y' type in 'x && y'
```

3) Perform the final correspondence of chromatographic peaks into features. 

```{r}
#' Final correspondence
std_data <- groupChromPeaks(std_data, param = pdp)
```


### 3.3.4 Gap filling

1) Perform gap filling to reduce number of peaks with NA values, by rechecking
   if there was a signal for each feature in each sample upon feature grouping.

```{r}
#' View total number of features
cat("Total number of features: ", 
      length(featureValues(std_data)), "\n")

#' View the number of missing features
cat("Missing values in features: ", 
      sum(is.na(featureValues(std_data))), "\n")

#' Perform gap filling
std_data <- fillChromPeaks(std_data, ChromPeakAreaParam())

#' View the number of missing features after gap filling
cat("Missing values in features after gap filling: ", 
      sum(is.na(featureValues(std_data))), "\n")
```

{
    *#FINDME: @jo/phili: why needed any of this below? 
    It is not really a 'step' you have to do?*
    
    Compare filled data to detected values.

```{r}
det <- featureValues(std_data, filled = FALSE)
fil <- featureValues(std_data)
fil[!is.na(det)] <- NA

boxplot(log2(det))
boxplot(log2(fil))
```

**Notes**: explain what we expect. why we compare low vs low and high vs high.
low: expect similar amount of filled values.
high : ?

compared filled vs detected for low

```{r}
a <- det[, which(grp == "low")]
b <- fil[, which(grp == "low")]
keep <- is.na(rowSums(det))

plot(rowMeans(log2(a), na.rm = TRUE)[keep],
     rowMeans(log2(b), na.rm = TRUE)[keep],
     xlab = "detected", ylab = "filled")
grid()
abline(0, 1, col = "grey")
```

Same for high

```{r}
a <- det[, which(grp == "high")]
b <- fil[, which(grp == "high")]
keep <- is.na(rowSums(det))

plot(rowMeans(log2(a), na.rm = TRUE)[keep],
     rowMeans(log2(b), na.rm = TRUE)[keep],
     xlab = "detected", ylab = "filled")
grid()
abline(0, 1, col = "grey")
```

}

## 3.4 Post processing

Following the preprocessing, the resulting feature table represents the
foundation for downstream statistical and biological interpretation. This
post-processing phase typically involves several key steps to refine and analyze
the data further. These include normalization of feature intensities across
samples to correct for systematic variation, removal of outlier features that
may represent technical artifacts, and assessment of sample variance to identify
trends or inconsistencies.

Subsequently, differential abundance analysis is performed to identify metabolic
features that distinguish phenotypes or experimental conditions of
interest. Multivariate statistical methods, such as principal component analysis
(PCA), hierarchical clustering, (orthogonal) partial least squares discriminant
analysis ((O)PLS-DA), linear regression-based approaches, or machine learning
algorithms are often employed to visualize global patterns and highlight
relevant features.

Finally, metabolite annotation of significant or interesting features is a
critical step for biological interpretation, enabling insights into the
metabolic pathways or processes involved. The exact post-processing pipeline may
vary depending on the research question, experimental design, and biological
matrix.

For practical code examples and guidance on the post processing, we refer the
reader to the many [example
workflows](https://rformassspectrometry.github.io/Metabonaut/index.html) of the
Metabonaut resource [@louail_metabonaut_2025], which provides detailed and
modular R-based workflows for each component of post-processing.


{
    *#FINDME @phili/jo , see notes below decision: to add or not?*

### 3.4.1 CV-based filtering

    1) xxx

```{r}
idx_low <- grp == "low"

res_cv <- data.frame(low = rowRsd(featureValues(std_data)[, idx_low]),
                     high = rowRsd(featureValues(std_data)[, !idx_low]))

dens <- apply(res_cv, 2, density, na.rm = TRUE)

plot(NA, xlim=range(sapply(dens, "[[", "x")), ylim=range(sapply(dens, "[[", "y")),
     xlab = "X", ylab = "Density", main = "Density Curves with Fill")
cols <- rainbow(length(dens))
mapply(function(d, col) {
  polygon(d$x, d$y, col=adjustcolor(col, alpha.f=0.5), border=col)
}, dens, cols)
abline(v = 0.3, col = "red", lty = 2, lwd = 2)
legend("topright", legend=names(dens), fill=adjustcolor(cols, alpha.f=0.5))
```

oh that would remove a lot of features.

We filter based on the Qcs samples. because in our dataset we have 2 types of
QCs we will do this in 2 steps (???)

```{r}
std_data_filt <- filterFeatures(std_data, RsdFilter(qcIndex = idx_low,
                                                   threshold = 0.3))

std_data_filt <- filterFeatures(std_data_filt, RsdFilter(qcIndex = !idx_low,
                                                   threshold = 0.3))
```

**Notes**: *Idk if you want to keep that. as you want !
Also there is a MissingValueFilter that could be nice here ? especially because
there is SO many features.
Maybe mention that usually we would run the CV one after normalisation no ?
mdg: I would remove this whole  3.4.1 section and only mention some text what
you can do and refer to the metabonaut repo.*

}


## 3.5 Data export

After completing preprocessing, and optionaly some further data filtering and/or
normalisation, the feature table can be exported for downstream analysis,
sharing, or archiving. This step ensures reproducibility and allows
interoperability with other software tools for statistical analysis or
metabolite identification. Below, we outline key procedures to export the
processed data, including both the feature intensity matrix and the MS/MS
spectra.

1) Export feature table as a `SummarizedExperiment` object (*see* **Note 9**). 

```{r}
#' Extract feature intensity results as a SummarizedExperiment
res <- quantify(std_data, method = "sum", filled = FALSE)

#' View the summary of the result object
res
```

2) Add a second assay containing missing-value filled data to
   `SummarizedExperiment` object. The additional assay contains filled intensity
   values (i.e., using gap-filling methods for missing peaks), so it can be used
   in downstream imputation-aware analysis.

```{r}
#' Add a second assay named 'raw_filled' with gap-filled intensities
assays(res)$raw_filled <- featureValues(std_data, method = "sum",
                                        filled = TRUE )

#' View different assays in the SummarizedExperiment object
assayNames(res)

#' View the top of the filled feature intensity matrix 
assay(res, "raw_filled") |> head()
```

3) Export the feature abundance matrix to a spreadsheet. For ease of data
   sharing or as input into external tools (e.g., Excel, Python), the feature
   intensity matrix can be exported as a .csv file.

```{r}
#' Write the filled feature intensity matrix to a CSV file
write.csv(assay(res, "raw_filled"),
         file = "data/standards_abundance.csv",
         row.names = TRUE)
```

4) Export the MS/MS spectra in MGF format. To support metabolite annotation in
   external tools (e.g., GNPS, MS-DIAL, or SIRIUS), the MS/MS spectra associated
   with detected MS features are exported to the Mascot Generic Format
   (MGF). The `MsBackendMgf()` backend function is used here to write the
   compatible files.

```{r}
#' Extract the MS/MS spectra corresponding to the chromatographic peaks
sp <- chromPeakSpectra(std_data)

#' Export the MS/MS spectra in .mgf format
export(sp, MsBackendMgf(), file = "data/std_spectra.mgf")
```


## 3.6 Session information

1) Print the R session info in the rendered Quarto document (*see* **Note 10**). 

```{r}
#' View R session info
sessionInfo()
```


# Notes

1) Writing workflow processes using Quarto documents (.qmd) supports literate
   programming by integrating code, text, and outputs in a single file. Quarto
   enables the creation of fully reproducible documents by embedding R code
   directly within narrative text, ensuring that results are dynamically
   generated from the most up-to-date data. This approach facilitates
   transparent, data-specific analysis workflows that are easy to audit, share,
   and update. By combining documentation and computation, Quarto helps
   streamline reporting, improve collaboration, and enhance the reproducibility
   of scientific and statistical analyses.

2) The workflow starts from vendor-neutral raw MS files, already converted from
   vendor-specific proprietary raw file formats (e.g., Thermo Fisher, Agilent,
   Bruker, Waters) to the open standard for MS data, i.e. the mzML format.
   Conversion of vendor-specific raw MS files is performed using *MSConvert*, a
   powerful and flexible tool from *ProteoWizard*. Using mzML as a standard
   format ensures that your MS data can be analyzed in a reproducible,
   tool-independent way.

3) R packages can be installed from the Comprehensive R Archive Network (CRAN)
   or from Bioconductor using the following commands:

```
#' Install CRAN packages
install.packages("knitr") 

#' Install Bioconductor packages
install.packages("BiocManager")
BiocManager::install("BiocStyle")
```

4) Alternatively to using the `read_xlsx()` function from the *readxl* R
   package, the same tabular information could also be defined directly in R as
   a `data.frame` object, but it is generally better, and more convenient to the
   user, to define and load such spreadsheet from as spreadsheet program.

5) For improved visualization, it is recommended to assign specific colors to
   individual samples as well as to sample groups. These color settings can be
   re-used in all plots during the analysis. Suggested color-coding schemes
   include: acquisition batch, injection order, sample type (e.g., QC, spiked,
   biological), and biological phenotypes of interest (e.g., sex, case-control
   status). These visual cues aid in assessing data quality and identifying
   potential systematic variation.

6) When no prior information is available about expected compounds, select
   high-intensity peaks from the BPC, extract full MS1 scans, and evaluate EICs
   to derive rt and *m/z* parameters empirically.

7) Accurate peak detection requires centroided data and a prerequisite for the
   chromatographic peak detection algorithm. If your data is not centroided, use
   MSConvert or a similar tool to convert profile mode data to centroid mode
   before importing it into R. *#FINDME @phili, maybe note to make centroid, or
   do you also do this in MSconvert?*

8) The *m/z* mass deviation is expressed as ppm and not in *m/z* difference (in
   Da) because the variation scales with the molecular mass of the compound. It
   is important to estimate the ppm also on large compounds, as the ppm
   increases with larger compounds. For example, if your highest *m/z* detected
   is 1000 Da, and you have an estimated error of 5 ppm, the largest possible
   *m/z* deviation allowed is 1000 ± 0.005 Da (*m/z* mass deviation = 1000 Da *
   5 ppm / 10^6), ranging from 999.995 to 1000.005 Da.

9) The SummarizedExperiment container is a robust Bioconductor class that holds
   both assay data (e.g., intensity values) and associated metadata (e.g.,
   feature and sample annotations). It provides a standardized format for
   integrating omics data and simplifies downstream manipulation or analysis.

10) Including a call to sessionInfo() at the end of the workflow is important
    for ensuring reproducibility, as it documents the versions of R and all
    loaded packages, as well as system-specific details that may influence
    results.


# References


